{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rich output 출력위치 설정\n",
    "# => 웹 브라우저에서 바로 봄\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# 색상 출력\n",
    "# 설치 방법 => pip install termcolor\n",
    "from termcolor import colored\n",
    "def c(tag, data, under=3):\n",
    "\n",
    "    val = colored(tag + ':\\n', 'magenta', attrs=['bold'])\n",
    "    t = type(data)\n",
    "    val += colored(t, 'magenta')\n",
    "    under = str(under)\n",
    "    \n",
    "    import numpy\n",
    "    if t is float or t is numpy.float64:\n",
    "        val += ('\\n{:.' + under + 'f}\\n').format(data)\n",
    "    else:\n",
    "        val += '\\n{}\\n'.format(data)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#184;background:#EEE\">2.3.4. 나이브 베이즈 분류기</span><br>\n",
    "\n",
    "- LogisticRegression과 LinearSVC보다 훈련 속도가 빠르지만 일반화 성능이 조금 떨어짐\n",
    "- 나이브 베이즈 분류기가 효과적인 이유\n",
    "    - 각 특성을 개별로 취급해 파라미터를 학습함\n",
    "    - 그리고 각 특성에서 클래스별 통계를 단순하게 취합<br><br>\n",
    "- scikit-learn 모델<br><br>\n",
    "    \n",
    "  - **BernoulliNB**\n",
    "    - (이산) 이진 데이터에 적용\n",
    "    - 각 클래스의 특성 중 0이 아닌 것이 몇 개인지 셈<br><br>\n",
    "    \n",
    "  - **MultinomialNB**\n",
    "    - (이산) 카운트 데이터에 적용 (예를 들면, 문장에 나타난 단어의 횟수)\n",
    "    - 클래스별로 특성의 평균을 계산<br><br>\n",
    "\n",
    "  - **GaussianNB**\n",
    "    - 연속 데이터에 적용\n",
    "    - 클래스별로 각 특성의 표준편차와 평균을 저장<br><br>\n",
    "    \n",
    "- **<span style=\"color:#C22\">베이즈 정리</span>**\n",
    "  - $\\displaystyle P(A∣B)= \\frac{P(A∩B)}{P(B)}=\\frac{P(A)P(B∣A)}{P(B)}$\n",
    "  - $\\displaystyle P(B∣A)= \\frac{P(B∩A)}{P(A)}=\\frac{P(B)P(A∣B)}{P(A)}$<br><br>\n",
    "<img src=\"../images/bayes_theorem.jpg\" style=\"margin:0;width:300px\"/><br>\n",
    "\n",
    "  \n",
    "- **<span style=\"color:#C22\">베이즈 분류</span>**<br><br>\n",
    "  - 입력 데이터의 내용이 $X=(x_1, x_2, ..., x_p)$로 주어졌을 때 $Y=k$일 확률\n",
    "    - $\\displaystyle P(Y = k ~|~ X_1 = x_1, X_2=x_2, ... , X_p = x_p)=P(Y = k) P(X_1 = x_1, X_2=x_2, ... , X_p = x_p~|~Y = k)$<br><br>\n",
    "  - 입력 데이터의 각 특성값의 조건부 분포가 서로 독립이라는 단순 베이즈 가정을 한다면\n",
    "    - $\\displaystyle P(Y = k ~|~ X_1 = x_1, X_2=x_2, ... , X_p = x_p)=P(Y = k) \\prod_{j=1}^{p}P(X_j = x_j~|~Y = k)$<br><br>\n",
    "  - 따라서, 입력 데이터의 내용이 $X=(x_1, x_2, ..., x_p)$로 주어졌을 때 $Y$ 예측 방법\n",
    "    - $\\displaystyle argmax_{k\\in Y} \\big{(}P(Y = k) \\prod_{j=1}^{p}P(X_j = x_j~|~Y = k)\\big{)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35mfeature counts:\n",
      "\u001b[0m\u001b[35m<class 'dict'>\u001b[0m\n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n",
      "\n",
      "\u001b[1m\u001b[35mprediction:\n",
      "\u001b[0m\u001b[35m<class 'numpy.ndarray'>\u001b[0m\n",
      "[0 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB 분류기만 해보도록 함\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train = np.array([[0, 1, 0, 1],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 0, 1, 0]])\n",
    "\n",
    "y_train = np.array([0, 1, 0, 1])\n",
    "\n",
    "# 특성 카운트를 직접 해보기\n",
    "counts = {}\n",
    "for label in np.unique(y_train):\n",
    "    # 특성이 있으면 다 1이기 때문에 그냥 더하면 됨\n",
    "    counts[label] = X_train[y_train == label].sum(axis=0)\n",
    "\n",
    "# 모델 생성 및 학습\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_test = [[0, 0, 0, 0],\n",
    "          [1, 1, 1, 1]]\n",
    "\n",
    "print(c('feature counts', counts))\n",
    "print(c('prediction', clf.predict(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **<span style=\"color:#C22\">장단점과 매개변수</span>**<br><br>\n",
    "    \n",
    "    - **설명**\n",
    "        1. BernoulliNB와 MultinomialNB는 alpah 값을 매개변수로 설정 가능\n",
    "        2. alpha 값만큼 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 추가\n",
    "        3. 이는 통계 데이터를 완만하게 해줌, alpha가 클수록 모델의 복잡도는 저하\n",
    "        4. GaussianNB는 주로 고차원 데이터 셋, 나머지는 텍스트와 같이 희소 데이터 셋\n",
    "        5. MultinomialNB는 매우 큰 문서들에게 효과적<br><br>\n",
    "    \n",
    "    - **강점**\n",
    "        1. 훈련과 예측 속도가 빠름\n",
    "        2. 희소 고차원 데이터 셋에서 잘 작동\n",
    "        3. 매개변수에 민감하지 않음\n",
    "        4. 다른 선형 모델로 하면 너무 오래 걸릴 때 종종 시도함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
